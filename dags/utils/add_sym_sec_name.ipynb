{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version : 3.4.1\n",
      "Number of CPU: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import input_file_name, lit, col, isnull\n",
    "from pyspark.sql import functions as F\n",
    "print(f\"PySpark Version : {pyspark.__version__}\")\n",
    "import multiprocessing\n",
    "print(f\"Number of CPU: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 22:22:22 WARN Utils: Your hostname, Hops-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.140 instead (on interface en0)\n",
      "23/07/28 22:22:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/28 22:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create a spark Context class, with custom config\n",
    "conf = SparkConf()\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create spark session\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"ETFs Spark Airflow Docker\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "existing_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), False),\n",
    "    StructField(\"Open\", FloatType(), False),\n",
    "    StructField(\"High\", FloatType(), False),\n",
    "    StructField(\"Low\", FloatType(), False),\n",
    "    StructField(\"Close\", FloatType(), False),\n",
    "    StructField(\"Adj Close\", FloatType(), False),\n",
    "    StructField(\"Volume\", FloatType(), False),\n",
    "    StructField(\"Symbol\", FloatType(), False),\n",
    "    StructField(\"Security Name\", FloatType(), False)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/stocks_etfs/A.csv\"\n",
    "stock_df = spark.read.csv(input_path, header=True, schema=existing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: float (nullable = true)\n",
      " |-- Security Name: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_symbol = spark.read.csv(\"../data/symbols_valid_meta.csv\", header=True)\n",
    "symbol_mapping = meta_symbol.select(\"Symbol\", \"Security Name\").rdd.collectAsMap()\n",
    "symbol_name = os.path.splitext(os.path.basename(input_path))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|  null|         null|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|  null|         null|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|  null|         null|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|  null|         null|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|  null|         null|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|  null|         null|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|  null|         null|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|  null|         null|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|  null|         null|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|  null|         null|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 01:25:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 9\n",
      "CSV file: file:///Users/hople/working_folder/Bootcamp_practices/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/Dockerize_entire_workflow/dags/data/stocks_etfs/A.csv\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.withColumn(\"Symbol\", F.lit(symbol_name))\n",
    "stock_df = stock_df.withColumn(\"Security Name\", F.lit(symbol_mapping.get(symbol_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.write.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|       Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|     A|Agilent Technolog...|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|     A|Agilent Technolog...|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|     A|Agilent Technolog...|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|     A|Agilent Technolog...|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|     A|Agilent Technolog...|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|     A|Agilent Technolog...|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|     A|Agilent Technolog...|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|     A|Agilent Technolog...|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|     A|Agilent Technolog...|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|     A|Agilent Technolog...|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Security Name: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.select(\"Security Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: string (nullable = false)\n",
      " |-- Security Name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from save_parquet import save_parquet\n",
    "import os\n",
    "\n",
    "#retain features columns\n",
    "features = ['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "#path to save processed dataset\n",
    "path = '../data/processed_stocks_etfs/'\n",
    "#read metal symbol files\n",
    "metal_symbol = pd.read_csv('../data/symbols_valid_meta.csv')\n",
    "metal_symbol = metal_symbol[['Symbol', 'Security Name']]\n",
    "#correct some wrong spelling, coresponding to Stock file name\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('$', '-',regex=False)\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('.V', '',regex=False)\n",
    "#creat mapping dictionary\n",
    "symbol_mapping = metal_symbol.set_index('Symbol').to_dict()['Security Name']\n",
    "\n",
    "def add_name(file):\n",
    "    #print(symbol_mapping)\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df = pd.read_csv(file)\n",
    "    df['Symbol'] = name\n",
    "    sec_name = symbol_mapping[name]\n",
    "    df['Security Name'] = sec_name\n",
    "    #print(sec_name)\n",
    "    #df.name = name\n",
    "    #return df\n",
    "    save_parquet(df[features], name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "stocks_path = '../data/stocks_etfs/'\n",
    "#path = '../data/processed_stocks_etfs/'\n",
    "from load_files import load_file\n",
    "#list of loaded csv files will split into n_processor, for parralezation process\n",
    "n_processor = cpu_count()\n",
    "#get batches of data, list of list\n",
    "preprocessing_list = load_file(n_processor, stocks_path, 'csv')\n",
    "\n",
    "\n",
    "def data_processing():\n",
    "    '''\n",
    "    Takes batch number as input\n",
    "    Map function add_name for every dataframe in batch number in preprocessing_list\n",
    "    '''\n",
    "    temp = list(map(add_name, preprocessing_list))\n",
    "\n",
    "#data_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/28 01:06:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "from load_files import load_file #function load files into batches\n",
    "\n",
    "#Create a spark Context class, with custom config to optimize the performance\n",
    "#conf.set('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 24)\n",
    "#conf.set('spark.sql.adaptive.coalescePartitions.parallelismFirst', 'false')\n",
    "#conf.set('spark.sql.files.minPartitionNum', 1)\n",
    "conf = SparkConf()\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "#conf.set('spark.sql.files.maxPartitionBytes', '500mb')\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "## Initialize SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"ETFs Spark Airflow Docker\").getOrCreate()\n",
    "\n",
    "\n",
    "#stock dir\n",
    "stocks_dir = \"../data/stocks_etfs\"\n",
    "#processed data dir\n",
    "processed_stocks_dir = \"../data/processed_stocks_etfs\"\n",
    "\n",
    "#Mapping dict\n",
    "meta_symbol = spark.read.csv(\"../data/symbols_valid_meta.csv\", header=True)\n",
    "symbol_mapping = meta_symbol.select(\"Symbol\", \"Security Name\").rdd.collectAsMap()\n",
    "\n",
    "#Define Schema for the data\n",
    "existing_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), False),\n",
    "    StructField(\"Open\", FloatType(), False),\n",
    "    StructField(\"High\", FloatType(), False),\n",
    "    StructField(\"Low\", FloatType(), False),\n",
    "    StructField(\"Close\", FloatType(), False),\n",
    "    StructField(\"Adj Close\", FloatType(), False),\n",
    "    StructField(\"Volume\", FloatType(), False),\n",
    "    StructField(\"Symbol\", FloatType(), False),\n",
    "    StructField(\"Security Name\", FloatType(), False)\n",
    "\n",
    "])\n",
    "\n",
    "def add_sym_sec_name(input_file):\n",
    "    \"\"\"\n",
    "    Function adds Symbol and Security Name to stock file\n",
    "    \"\"\"\n",
    "    # Read data from CSV into the DataFrame using the existing schema\n",
    "    stock_df = spark.read.csv(input_file, header=True, schema=existing_schema)\n",
    "\n",
    "    # Get Symbol name from input file\n",
    "    symbol_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Adding Symbol and Security Name\n",
    "    stock_df = stock_df.withColumn(\"Symbol\", F.lit(symbol_name))\n",
    "    stock_df = stock_df.withColumn(\"Security Name\", F.lit(symbol_mapping.get(symbol_name)))\n",
    "\n",
    "    # Save the preprocessed data to a parquet file\n",
    "    #output_file = os.path.join(processed_stocks_dir, f\"{symbol_name}_preprocessed.parquet\")\n",
    "    output_file = f\"{processed_stocks_dir}/{symbol_name}_preprocessed.parquet\"\n",
    "    stock_df.write.parquet(output_file, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "def preprocessing_data():\n",
    "    '''\n",
    "    Takes batch number as input\n",
    "    Map function add_sym_sec_name for every dataframe in batch number in preprocessing_list\n",
    "    '''\n",
    "    #list of loaded csv files will split into n_processor, for parralezation process in Airflow\n",
    "    n_processor = cpu_count()\n",
    "    #get batches of data\n",
    "    preprocessing_list = load_file(n_processor, stocks_dir, 'csv')\n",
    "    #temp = list(map(add_sym_sec_name, ('../data/stocks_etfs/A.csv')))\n",
    "    #print(preprocessing_list)\n",
    "\n",
    "preprocessing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mstr\u001b[39m(preprocessing_list[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing_list' is not defined"
     ]
    }
   ],
   "source": [
    "str(preprocessing_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|Symbol|Security Name|\n",
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "|1988-02-04|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 38700.0|  null|         null|\n",
      "|1988-02-05|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868|606300.0|  null|         null|\n",
      "|1988-02-08|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 19000.0|  null|         null|\n",
      "|1988-02-09|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 23100.0|  null|         null|\n",
      "|1988-02-10|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 10000.0|  null|         null|\n",
      "|1988-02-11|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 10900.0|  null|         null|\n",
      "|1988-02-12|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868|     0.0|  null|         null|\n",
      "|1988-02-16|       0.0|0.61728394|0.52469134|0.52469134|0.40048075| 66700.0|  null|         null|\n",
      "|1988-02-17|0.52469134|0.52469134|0.52469134|0.52469134|0.40048075|     0.0|  null|         null|\n",
      "|1988-02-18|       0.0|0.61728394|0.49382716|0.49382716| 0.3769232| 21100.0|  null|         null|\n",
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 00:37:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 9\n",
      "CSV file: file:///Users/hople/working_folder/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/dags/data/stocks_etfs/IPAR.csv\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(str(preprocessing_list[0]), header=True, schema=existing_schema).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AAL_preprocessed'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_files import load_file\n",
    "from multiprocessing import cpu_count\n",
    "stocks_dir = '../data/stocks_etfs/'\n",
    "n_processor = cpu_count()\n",
    "#get batches of data\n",
    "preprocessing_list = load_file(n_processor, '../data/processed_stocks_etfs/', 'parquet')\n",
    "\n",
    "preprocessing_list[0][0].stem\n",
    "#pd.read_parquet(preprocessing_list[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/30 20:23:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from sparksession import initilize_sparksession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, DateType\n",
    "from pyspark.sql import functions as F\n",
    "spark = initilize_sparksession()\n",
    "\n",
    "# Define the schema for the Parquet file\n",
    "custom_schema = StructType([\n",
    "    StructField(\"Symbol\", StringType(), True),\n",
    "    StructField(\"Security Name\", StringType(), True),\n",
    "    StructField(\"Date\", DateType(), True),\n",
    "    StructField(\"Open\", FloatType(), True),\n",
    "    StructField(\"High\", FloatType(), True),\n",
    "    StructField(\"Low\", FloatType(), True),\n",
    "    StructField(\"Close\", FloatType(), True),\n",
    "    StructField(\"Adj Close\", FloatType(), True),\n",
    "    StructField(\"Volume\", FloatType(), True),\n",
    "    StructField(\"vol_moving_avg\", FloatType(), True),\n",
    "    StructField(\"adj_close_rolling_med\", FloatType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "A_stocks = spark.read.parquet('../data/processed_stocks_etfs/A_preprocessed.parquet/', scheme=custom_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate volume moving average using Window function for the last 30 days, including the current row\n",
    "window_spec = Window.partitionBy(F.lit(0)).orderBy(F.col('Date')).rowsBetween(-29, 0)\n",
    "A_stocks = A_stocks.withColumn('vol_moving_avg', F.avg('Volume').over(window_spec))\n",
    "A_stocks = A_stocks.withColumn('vol_moving_avg', F.round('vol_moving_avg', 0))\n",
    "\n",
    "#drop the first 30 days\n",
    "A_stocks = A_stocks.withColumn(\"counter\", F.monotonically_increasing_id())\n",
    "w = Window.partitionBy(F.lit(0)).orderBy(\"counter\")\n",
    "A_stocks = A_stocks.withColumn(\"index\", F.row_number().over(w))\n",
    "A_stocks = A_stocks.filter(F.col(\"index\") >= 30)\n",
    "A_stocks = A_stocks.drop(\"counter\", \"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [Symbol#0, Security Name#1, Date#2, Open#3, High#4, Low#5, Close#6, Adj Close#7, Volume#8, vol_moving_avg#31]\n",
      "   +- Filter (index#55 >= 30)\n",
      "      +- Window [row_number() windowspecdefinition(0, counter#42L ASC NULLS FIRST, specifiedwindowframe(RowFrame, unboundedpreceding$(), currentrow$())) AS index#55], [0], [counter#42L ASC NULLS FIRST]\n",
      "         +- Sort [0 ASC NULLS FIRST, counter#42L ASC NULLS FIRST], false, 0\n",
      "            +- Project [Symbol#0, Security Name#1, Date#2, Open#3, High#4, Low#5, Close#6, Adj Close#7, Volume#8, round(vol_moving_avg#19, 0) AS vol_moving_avg#31, monotonically_increasing_id() AS counter#42L]\n",
      "               +- Window [avg(Volume#8) windowspecdefinition(0, Date#2 ASC NULLS FIRST, specifiedwindowframe(RowFrame, -29, currentrow$())) AS vol_moving_avg#19], [0], [Date#2 ASC NULLS FIRST]\n",
      "                  +- Sort [0 ASC NULLS FIRST, Date#2 ASC NULLS FIRST], false, 0\n",
      "                     +- Exchange hashpartitioning(0, 8), ENSURE_REQUIREMENTS, [plan_id=24]\n",
      "                        +- FileScan parquet [Symbol#0,Security Name#1,Date#2,Open#3,High#4,Low#5,Close#6,Adj Close#7,Volume#8] Batched: true, DataFilters: [], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/Users/hople/working_folder/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/dags..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Symbol:string,Security Name:string,Date:string,Open:float,High:float,Low:float,Close:float...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "A_stocks.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_stocks.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security Name</th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>vol_moving_avg</th>\n",
       "      <th>adj_close_rolling_med</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>1999-12-31</td>\n",
       "      <td>56.866951</td>\n",
       "      <td>57.179901</td>\n",
       "      <td>54.542202</td>\n",
       "      <td>55.302216</td>\n",
       "      <td>47.562416</td>\n",
       "      <td>1931100</td>\n",
       "      <td>5.739950e+06</td>\n",
       "      <td>2937500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2000-01-03</td>\n",
       "      <td>56.330471</td>\n",
       "      <td>56.464592</td>\n",
       "      <td>48.193848</td>\n",
       "      <td>51.502148</td>\n",
       "      <td>44.294170</td>\n",
       "      <td>4674300</td>\n",
       "      <td>3.810883e+06</td>\n",
       "      <td>2937500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2000-01-04</td>\n",
       "      <td>48.730328</td>\n",
       "      <td>49.266811</td>\n",
       "      <td>46.316166</td>\n",
       "      <td>47.567955</td>\n",
       "      <td>40.910591</td>\n",
       "      <td>4765000</td>\n",
       "      <td>3.461913e+06</td>\n",
       "      <td>2937500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2000-01-05</td>\n",
       "      <td>47.389126</td>\n",
       "      <td>47.567955</td>\n",
       "      <td>43.141987</td>\n",
       "      <td>44.617310</td>\n",
       "      <td>38.372894</td>\n",
       "      <td>5758600</td>\n",
       "      <td>3.434607e+06</td>\n",
       "      <td>2937500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2000-01-06</td>\n",
       "      <td>44.080830</td>\n",
       "      <td>44.349072</td>\n",
       "      <td>41.577251</td>\n",
       "      <td>42.918453</td>\n",
       "      <td>36.911816</td>\n",
       "      <td>2534400</td>\n",
       "      <td>3.319900e+06</td>\n",
       "      <td>2750800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5119</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>74.449997</td>\n",
       "      <td>69.650002</td>\n",
       "      <td>73.720001</td>\n",
       "      <td>73.532867</td>\n",
       "      <td>3267500</td>\n",
       "      <td>3.201053e+06</td>\n",
       "      <td>3003400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5120</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>71.550003</td>\n",
       "      <td>73.209999</td>\n",
       "      <td>70.279999</td>\n",
       "      <td>70.910004</td>\n",
       "      <td>70.730003</td>\n",
       "      <td>1829800</td>\n",
       "      <td>3.208837e+06</td>\n",
       "      <td>3003400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5121</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>71.059998</td>\n",
       "      <td>73.180000</td>\n",
       "      <td>71.059998</td>\n",
       "      <td>72.669998</td>\n",
       "      <td>72.669998</td>\n",
       "      <td>1486200</td>\n",
       "      <td>3.192863e+06</td>\n",
       "      <td>3003400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5122</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>72.339996</td>\n",
       "      <td>72.800003</td>\n",
       "      <td>70.500000</td>\n",
       "      <td>71.620003</td>\n",
       "      <td>71.620003</td>\n",
       "      <td>1822100</td>\n",
       "      <td>3.157850e+06</td>\n",
       "      <td>3003400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5123</th>\n",
       "      <td>A</td>\n",
       "      <td>Agilent Technologies, Inc. Common Stock</td>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>69.470001</td>\n",
       "      <td>70.230003</td>\n",
       "      <td>68.150002</td>\n",
       "      <td>68.919998</td>\n",
       "      <td>68.919998</td>\n",
       "      <td>2173600</td>\n",
       "      <td>3.072260e+06</td>\n",
       "      <td>2974750.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5095 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Symbol                            Security Name       Date       Open  \\\n",
       "29        A  Agilent Technologies, Inc. Common Stock 1999-12-31  56.866951   \n",
       "30        A  Agilent Technologies, Inc. Common Stock 2000-01-03  56.330471   \n",
       "31        A  Agilent Technologies, Inc. Common Stock 2000-01-04  48.730328   \n",
       "32        A  Agilent Technologies, Inc. Common Stock 2000-01-05  47.389126   \n",
       "33        A  Agilent Technologies, Inc. Common Stock 2000-01-06  44.080830   \n",
       "...     ...                                      ...        ...        ...   \n",
       "5119      A  Agilent Technologies, Inc. Common Stock 2020-03-26  70.000000   \n",
       "5120      A  Agilent Technologies, Inc. Common Stock 2020-03-27  71.550003   \n",
       "5121      A  Agilent Technologies, Inc. Common Stock 2020-03-30  71.059998   \n",
       "5122      A  Agilent Technologies, Inc. Common Stock 2020-03-31  72.339996   \n",
       "5123      A  Agilent Technologies, Inc. Common Stock 2020-04-01  69.470001   \n",
       "\n",
       "           High        Low      Close  Adj Close   Volume  vol_moving_avg  \\\n",
       "29    57.179901  54.542202  55.302216  47.562416  1931100    5.739950e+06   \n",
       "30    56.464592  48.193848  51.502148  44.294170  4674300    3.810883e+06   \n",
       "31    49.266811  46.316166  47.567955  40.910591  4765000    3.461913e+06   \n",
       "32    47.567955  43.141987  44.617310  38.372894  5758600    3.434607e+06   \n",
       "33    44.349072  41.577251  42.918453  36.911816  2534400    3.319900e+06   \n",
       "...         ...        ...        ...        ...      ...             ...   \n",
       "5119  74.449997  69.650002  73.720001  73.532867  3267500    3.201053e+06   \n",
       "5120  73.209999  70.279999  70.910004  70.730003  1829800    3.208837e+06   \n",
       "5121  73.180000  71.059998  72.669998  72.669998  1486200    3.192863e+06   \n",
       "5122  72.800003  70.500000  71.620003  71.620003  1822100    3.157850e+06   \n",
       "5123  70.230003  68.150002  68.919998  68.919998  2173600    3.072260e+06   \n",
       "\n",
       "      adj_close_rolling_med  \n",
       "29                2937500.0  \n",
       "30                2937500.0  \n",
       "31                2937500.0  \n",
       "32                2937500.0  \n",
       "33                2750800.0  \n",
       "...                     ...  \n",
       "5119              3003400.0  \n",
       "5120              3003400.0  \n",
       "5121              3003400.0  \n",
       "5122              3003400.0  \n",
       "5123              2974750.0  \n",
       "\n",
       "[5095 rows x 11 columns]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet('../data/featuresAdded_stocks_etfs/A.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
