{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version : 3.4.1\n",
      "Number of CPU: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import input_file_name, lit, col, isnull\n",
    "from pyspark.sql import functions as F\n",
    "print(f\"PySpark Version : {pyspark.__version__}\")\n",
    "import multiprocessing\n",
    "print(f\"Number of CPU: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 22:22:22 WARN Utils: Your hostname, Hops-MacBook-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.0.140 instead (on interface en0)\n",
      "23/07/28 22:22:22 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/28 22:22:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Create a spark Context class, with custom config\n",
    "conf = SparkConf()\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create spark session\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"ETFs Spark Airflow Docker\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "existing_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), False),\n",
    "    StructField(\"Open\", FloatType(), False),\n",
    "    StructField(\"High\", FloatType(), False),\n",
    "    StructField(\"Low\", FloatType(), False),\n",
    "    StructField(\"Close\", FloatType(), False),\n",
    "    StructField(\"Adj Close\", FloatType(), False),\n",
    "    StructField(\"Volume\", FloatType(), False),\n",
    "    StructField(\"Symbol\", FloatType(), False),\n",
    "    StructField(\"Security Name\", FloatType(), False)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/stocks_etfs/A.csv\"\n",
    "stock_df = spark.read.csv(input_path, header=True, schema=existing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: float (nullable = true)\n",
      " |-- Security Name: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_symbol = spark.read.csv(\"../data/symbols_valid_meta.csv\", header=True)\n",
    "symbol_mapping = meta_symbol.select(\"Symbol\", \"Security Name\").rdd.collectAsMap()\n",
    "symbol_name = os.path.splitext(os.path.basename(input_path))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|  null|         null|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|  null|         null|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|  null|         null|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|  null|         null|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|  null|         null|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|  null|         null|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|  null|         null|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|  null|         null|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|  null|         null|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|  null|         null|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 01:25:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 9\n",
      "CSV file: file:///Users/hople/working_folder/Bootcamp_practices/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/Dockerize_entire_workflow/dags/data/stocks_etfs/A.csv\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.withColumn(\"Symbol\", F.lit(symbol_name))\n",
    "stock_df = stock_df.withColumn(\"Security Name\", F.lit(symbol_mapping.get(symbol_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.write.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|       Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|     A|Agilent Technolog...|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|     A|Agilent Technolog...|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|     A|Agilent Technolog...|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|     A|Agilent Technolog...|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|     A|Agilent Technolog...|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|     A|Agilent Technolog...|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|     A|Agilent Technolog...|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|     A|Agilent Technolog...|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|     A|Agilent Technolog...|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|     A|Agilent Technolog...|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Security Name: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.select(\"Security Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: string (nullable = false)\n",
      " |-- Security Name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from save_parquet import save_parquet\n",
    "import os\n",
    "\n",
    "#retain features columns\n",
    "features = ['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "#path to save processed dataset\n",
    "path = '../data/processed_stocks_etfs/'\n",
    "#read metal symbol files\n",
    "metal_symbol = pd.read_csv('../data/symbols_valid_meta.csv')\n",
    "metal_symbol = metal_symbol[['Symbol', 'Security Name']]\n",
    "#correct some wrong spelling, coresponding to Stock file name\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('$', '-',regex=False)\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('.V', '',regex=False)\n",
    "#creat mapping dictionary\n",
    "symbol_mapping = metal_symbol.set_index('Symbol').to_dict()['Security Name']\n",
    "\n",
    "def add_name(file):\n",
    "    #print(symbol_mapping)\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df = pd.read_csv(file)\n",
    "    df['Symbol'] = name\n",
    "    sec_name = symbol_mapping[name]\n",
    "    df['Security Name'] = sec_name\n",
    "    #print(sec_name)\n",
    "    #df.name = name\n",
    "    #return df\n",
    "    save_parquet(df[features], name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "stocks_path = '../data/stocks_etfs/'\n",
    "#path = '../data/processed_stocks_etfs/'\n",
    "from load_files import load_file\n",
    "#list of loaded csv files will split into n_processor, for parralezation process\n",
    "n_processor = cpu_count()\n",
    "#get batches of data, list of list\n",
    "preprocessing_list = load_file(n_processor, stocks_path, 'csv')\n",
    "\n",
    "\n",
    "def data_processing():\n",
    "    '''\n",
    "    Takes batch number as input\n",
    "    Map function add_name for every dataframe in batch number in preprocessing_list\n",
    "    '''\n",
    "    temp = list(map(add_name, preprocessing_list))\n",
    "\n",
    "#data_processing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/07/28 01:06:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import functions as F\n",
    "import os\n",
    "from multiprocessing import cpu_count\n",
    "from load_files import load_file #function load files into batches\n",
    "\n",
    "#Create a spark Context class, with custom config to optimize the performance\n",
    "#conf.set('spark.sql.adaptive.coalescePartitions.initialPartitionNum', 24)\n",
    "#conf.set('spark.sql.adaptive.coalescePartitions.parallelismFirst', 'false')\n",
    "#conf.set('spark.sql.files.minPartitionNum', 1)\n",
    "conf = SparkConf()\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "#conf.set('spark.sql.files.maxPartitionBytes', '500mb')\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "sc = SparkContext.getOrCreate(conf)\n",
    "\n",
    "## Initialize SparkSession\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"ETFs Spark Airflow Docker\").getOrCreate()\n",
    "\n",
    "\n",
    "#stock dir\n",
    "stocks_dir = \"../data/stocks_etfs\"\n",
    "#processed data dir\n",
    "processed_stocks_dir = \"../data/processed_stocks_etfs\"\n",
    "\n",
    "#Mapping dict\n",
    "meta_symbol = spark.read.csv(\"../data/symbols_valid_meta.csv\", header=True)\n",
    "symbol_mapping = meta_symbol.select(\"Symbol\", \"Security Name\").rdd.collectAsMap()\n",
    "\n",
    "#Define Schema for the data\n",
    "existing_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), False),\n",
    "    StructField(\"Open\", FloatType(), False),\n",
    "    StructField(\"High\", FloatType(), False),\n",
    "    StructField(\"Low\", FloatType(), False),\n",
    "    StructField(\"Close\", FloatType(), False),\n",
    "    StructField(\"Adj Close\", FloatType(), False),\n",
    "    StructField(\"Volume\", FloatType(), False),\n",
    "    StructField(\"Symbol\", FloatType(), False),\n",
    "    StructField(\"Security Name\", FloatType(), False)\n",
    "\n",
    "])\n",
    "\n",
    "def add_sym_sec_name(input_file):\n",
    "    \"\"\"\n",
    "    Function adds Symbol and Security Name to stock file\n",
    "    \"\"\"\n",
    "    # Read data from CSV into the DataFrame using the existing schema\n",
    "    stock_df = spark.read.csv(input_file, header=True, schema=existing_schema)\n",
    "\n",
    "    # Get Symbol name from input file\n",
    "    symbol_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "\n",
    "    # Adding Symbol and Security Name\n",
    "    stock_df = stock_df.withColumn(\"Symbol\", F.lit(symbol_name))\n",
    "    stock_df = stock_df.withColumn(\"Security Name\", F.lit(symbol_mapping.get(symbol_name)))\n",
    "\n",
    "    # Save the preprocessed data to a parquet file\n",
    "    output_file = os.path.join(processed_stocks_dir, f\"{symbol_name}_preprocessed.parquet\")\n",
    "    stock_df.write.parquet(output_file, mode=\"overwrite\")\n",
    "\n",
    "\n",
    "def preprocessing_data():\n",
    "    '''\n",
    "    Takes batch number as input\n",
    "    Map function add_sym_sec_name for every dataframe in batch number in preprocessing_list\n",
    "    '''\n",
    "    #list of loaded csv files will split into n_processor, for parralezation process in Airflow\n",
    "    n_processor = cpu_count()\n",
    "    #get batches of data\n",
    "    preprocessing_list = load_file(n_processor, stocks_dir, 'csv')\n",
    "    #temp = list(map(add_sym_sec_name, ('../data/stocks_etfs/A.csv')))\n",
    "    #print(preprocessing_list)\n",
    "\n",
    "preprocessing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessing_list' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mstr\u001b[39m(preprocessing_list[\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocessing_list' is not defined"
     ]
    }
   ],
   "source": [
    "str(preprocessing_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "|      Date|      Open|      High|       Low|     Close| Adj Close|  Volume|Symbol|Security Name|\n",
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "|1988-02-04|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 38700.0|  null|         null|\n",
      "|1988-02-05|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868|606300.0|  null|         null|\n",
      "|1988-02-08|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 19000.0|  null|         null|\n",
      "|1988-02-09|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 23100.0|  null|         null|\n",
      "|1988-02-10|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 10000.0|  null|         null|\n",
      "|1988-02-11|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868| 10900.0|  null|         null|\n",
      "|1988-02-12|       0.0|0.61728394| 0.5555556| 0.5555556|0.42403868|     0.0|  null|         null|\n",
      "|1988-02-16|       0.0|0.61728394|0.52469134|0.52469134|0.40048075| 66700.0|  null|         null|\n",
      "|1988-02-17|0.52469134|0.52469134|0.52469134|0.52469134|0.40048075|     0.0|  null|         null|\n",
      "|1988-02-18|       0.0|0.61728394|0.49382716|0.49382716| 0.3769232| 21100.0|  null|         null|\n",
      "+----------+----------+----------+----------+----------+----------+--------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/28 00:37:48 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 9\n",
      "CSV file: file:///Users/hople/working_folder/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/dags/data/stocks_etfs/IPAR.csv\n"
     ]
    }
   ],
   "source": [
    "spark.read.csv(str(preprocessing_list[0]), header=True, schema=existing_schema).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from load_files import load_file\n",
    "from multiprocessing import cpu_count\n",
    "stocks_dir = '../data/stocks_etfs/'\n",
    "n_processor = cpu_count()\n",
    "#get batches of data\n",
    "preprocessing_list = load_file(n_processor, stocks_dir, 'csv')\n",
    "\n",
    "len(preprocessing_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1988-02-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>38700.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1988-02-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>606300.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1988-02-08</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>19000.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988-02-09</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>23100.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988-02-10</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.617284</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.424039</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8099</th>\n",
       "      <td>2020-03-26</td>\n",
       "      <td>46.139999</td>\n",
       "      <td>50.970001</td>\n",
       "      <td>46.139999</td>\n",
       "      <td>49.240002</td>\n",
       "      <td>48.900555</td>\n",
       "      <td>198400.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8100</th>\n",
       "      <td>2020-03-27</td>\n",
       "      <td>46.790001</td>\n",
       "      <td>49.220001</td>\n",
       "      <td>46.180000</td>\n",
       "      <td>47.869999</td>\n",
       "      <td>47.539997</td>\n",
       "      <td>99800.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8101</th>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>46.910000</td>\n",
       "      <td>50.419998</td>\n",
       "      <td>46.910000</td>\n",
       "      <td>49.770000</td>\n",
       "      <td>49.770000</td>\n",
       "      <td>106000.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8102</th>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>48.919998</td>\n",
       "      <td>49.650002</td>\n",
       "      <td>44.509998</td>\n",
       "      <td>46.349998</td>\n",
       "      <td>46.349998</td>\n",
       "      <td>186900.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8103</th>\n",
       "      <td>2020-04-01</td>\n",
       "      <td>44.240002</td>\n",
       "      <td>45.849998</td>\n",
       "      <td>39.480000</td>\n",
       "      <td>40.320000</td>\n",
       "      <td>40.320000</td>\n",
       "      <td>148000.0</td>\n",
       "      <td>IPAR</td>\n",
       "      <td>Inter Parfums, Inc. - Common Stock</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8104 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date       Open       High        Low      Close  Adj Close  \\\n",
       "0     1988-02-04   0.000000   0.617284   0.555556   0.555556   0.424039   \n",
       "1     1988-02-05   0.000000   0.617284   0.555556   0.555556   0.424039   \n",
       "2     1988-02-08   0.000000   0.617284   0.555556   0.555556   0.424039   \n",
       "3     1988-02-09   0.000000   0.617284   0.555556   0.555556   0.424039   \n",
       "4     1988-02-10   0.000000   0.617284   0.555556   0.555556   0.424039   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "8099  2020-03-26  46.139999  50.970001  46.139999  49.240002  48.900555   \n",
       "8100  2020-03-27  46.790001  49.220001  46.180000  47.869999  47.539997   \n",
       "8101  2020-03-30  46.910000  50.419998  46.910000  49.770000  49.770000   \n",
       "8102  2020-03-31  48.919998  49.650002  44.509998  46.349998  46.349998   \n",
       "8103  2020-04-01  44.240002  45.849998  39.480000  40.320000  40.320000   \n",
       "\n",
       "        Volume Symbol                       Security Name  \n",
       "0      38700.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "1     606300.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "2      19000.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "3      23100.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "4      10000.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "...        ...    ...                                 ...  \n",
       "8099  198400.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "8100   99800.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "8101  106000.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "8102  186900.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "8103  148000.0   IPAR  Inter Parfums, Inc. - Common Stock  \n",
       "\n",
       "[8104 rows x 9 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.read_parquet('../data/processed_stocks_etfs/IPAR_preprocessed.parquet/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
