{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version : 3.4.1\n",
      "Number of CPU: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import input_file_name, lit, col, isnull\n",
    "from pyspark.sql import functions as F\n",
    "print(f\"PySpark Version : {pyspark.__version__}\")\n",
    "import multiprocessing\n",
    "print(f\"Number of CPU: {multiprocessing.cpu_count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a spark Context class, with custom config\n",
    "conf = SparkConf()\n",
    "conf.set('spark.default.parallelism', 700)\n",
    "conf.set('spark.sql.shuffle.partitions', 700)\n",
    "conf.set('spark.driver.memory', '30g')\n",
    "conf.set('spark.driver.cores', 8)\n",
    "conf.set('spark.executor.cores', 8)\n",
    "conf.set('spark.executor.memory', '30g')\n",
    "sc = SparkContext.getOrCreate(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create spark session\n",
    "spark = SparkSession.builder.master('local[*]').\\\n",
    "                config('spark.sql.debug.maxToStringFields', '100').\\\n",
    "                appName(\"ETFs Spark Airflow Docker\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "existing_schema = StructType([\n",
    "    StructField(\"Date\", StringType(), False),\n",
    "    StructField(\"Open\", FloatType(), False),\n",
    "    StructField(\"High\", FloatType(), False),\n",
    "    StructField(\"Low\", FloatType(), False),\n",
    "    StructField(\"Close\", FloatType(), False),\n",
    "    StructField(\"Adj Close\", FloatType(), False),\n",
    "    StructField(\"Volume\", FloatType(), False),\n",
    "    StructField(\"Symbol\", FloatType(), False),\n",
    "    StructField(\"Security Name\", FloatType(), False)\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../data/stocks_etfs/A.csv\"\n",
    "stock_df = spark.read.csv(input_path, header=True, schema=existing_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: float (nullable = true)\n",
      " |-- Security Name: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_symbol = spark.read.csv(\"../data/symbols_valid_meta.csv\", header=True)\n",
    "symbol_mapping = meta_symbol.select(\"Symbol\", \"Security Name\").rdd.collectAsMap()\n",
    "symbol_name = os.path.splitext(os.path.basename(input_path))[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|  null|         null|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|  null|         null|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|  null|         null|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|  null|         null|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|  null|         null|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|  null|         null|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|  null|         null|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|  null|         null|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|  null|         null|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|  null|         null|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+-------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/07/26 01:25:58 WARN CSVHeaderChecker: Number of column in CSV header is not equal to number of fields in the schema:\n",
      " Header length: 7, schema size: 9\n",
      "CSV file: file:///Users/hople/working_folder/Bootcamp_practices/ML_PIPELINE_AIRFLOW_SPARK_DOCKER/Dockerize_entire_workflow/dags/data/stocks_etfs/A.csv\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df = stock_df.withColumn(\"Symbol\", F.lit(symbol_name))\n",
    "stock_df = stock_df.withColumn(\"Security Name\", F.lit(symbol_mapping.get(symbol_name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_df.write.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|      Date|     Open|     High|      Low|    Close|Adj Close|   Volume|Symbol|       Security Name|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "|1999-11-18|32.546494| 35.76538|28.612303|31.473534|27.068665|6.25463E7|     A|Agilent Technolog...|\n",
      "|1999-11-19| 30.71352|30.758226|28.478184|28.880543|24.838577|1.52341E7|     A|Agilent Technolog...|\n",
      "|1999-11-22|29.551144|31.473534| 28.65701|31.473534|27.068665|6577800.0|     A|Agilent Technolog...|\n",
      "|1999-11-23|30.400572|31.205294|28.612303|28.612303| 24.60788|5975600.0|     A|Agilent Technolog...|\n",
      "|1999-11-24|28.701717| 29.99821|28.612303|29.372318|25.261524|4843200.0|     A|Agilent Technolog...|\n",
      "|1999-11-26|29.238197|29.685265|29.148785|29.461731|25.338428|1729400.0|     A|Agilent Technolog...|\n",
      "|1999-11-29| 29.32761|30.355865|29.014664|30.132332|25.915169|4074700.0|     A|Agilent Technolog...|\n",
      "|1999-11-30| 30.04292| 30.71352|29.282904|30.177038|25.953619|4310000.0|     A|Agilent Technolog...|\n",
      "|1999-12-01|30.177038|31.071173|29.953505| 30.71352|26.415012|2957300.0|     A|Agilent Technolog...|\n",
      "|1999-12-02|31.294706|32.188843|30.892345|31.562946|27.145563|3069800.0|     A|Agilent Technolog...|\n",
      "+----------+---------+---------+---------+---------+---------+---------+------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Security Name: string]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_df.select(\"Security Name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: string (nullable = true)\n",
      " |-- Open: float (nullable = true)\n",
      " |-- High: float (nullable = true)\n",
      " |-- Low: float (nullable = true)\n",
      " |-- Close: float (nullable = true)\n",
      " |-- Adj Close: float (nullable = true)\n",
      " |-- Volume: float (nullable = true)\n",
      " |-- Symbol: string (nullable = false)\n",
      " |-- Security Name: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stock_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "def load_file(n, path_file, file_format, press='*.'):\n",
    "    \"\"\"\n",
    "    Function devide list of csv files into list of batches, each batch contain list of csv files\n",
    "    \"\"\"\n",
    "    n = int(n)\n",
    "    ls_1 = list(Path(path_file).glob(press+file_format)) \n",
    "    return ls_1#list(map(list,np.array_split(ls_1, n)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from save_parquet import save_parquet\n",
    "import os\n",
    "\n",
    "#retain features columns\n",
    "features = ['Symbol', 'Security Name', 'Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n",
    "#path to save processed dataset\n",
    "path = '../data/processed_stocks_etfs/'\n",
    "\n",
    "def add_name(file, symbol_mapping):\n",
    "    name = os.path.splitext(os.path.basename(file))[0]\n",
    "    df = pd.read_csv(file)\n",
    "    df['Symbol'] = name\n",
    "    sec_name = symbol_mapping[name]\n",
    "    df['Security Name'] = sec_name\n",
    "    #df.name = name\n",
    "    #return df\n",
    "    save_parquet(df[features], name, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "\n",
    "stocks_path = '../data/stocks_etfs/'\n",
    "#path = '../data/processed_stocks_etfs/'\n",
    "#read metal symbol files\n",
    "metal_symbol = pd.read_csv('../data/symbols_valid_meta.csv')\n",
    "metal_symbol = metal_symbol[['Symbol', 'Security Name']]\n",
    "#correct some wrong spelling, coresponding to Stock file name\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('$', '-',regex=False)\n",
    "metal_symbol['Symbol'] = metal_symbol['Symbol'].str.replace('.V', '',regex=False)\n",
    "#creat mapping dictionary\n",
    "#symbol_mapping = metal_symbol.set_index('Symbol').to_dict()['Security Name']\n",
    "symbol_mapping = metal_symbol.set_index('Symbol').to_dict()['Security Name']\n",
    "\n",
    "#list of loaded csv files will split into n_processor, for parralezation process\n",
    "n_processor = cpu_count()\n",
    "#get batches of data, list of list\n",
    "preprocessing_list = load_file(n_processor, stocks_path, 'csv')\n",
    "\n",
    "\n",
    "def data_processing(preprocessing_list, symbol_mapping):\n",
    "    '''\n",
    "    Takes batch number as input\n",
    "    Map function add_name for every dataframe in batch number in preprocessing_list\n",
    "    '''\n",
    "    temp = list(map(add_name, preprocessing_list, symbol_mapping))\n",
    "\n",
    "data_processing(preprocessing_list=preprocessing_list, symbol_mapping=[symbol_mapping])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Agilent Technologies, Inc. Common Stock'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "symbol_mapping[\"A\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "work_sample",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
